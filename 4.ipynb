{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd81c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data import X_train, X_test, y_train, y_test\n",
    "from tree import print_tree\n",
    "from random_forest import RF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b558f30f",
   "metadata": {},
   "source": [
    "La clase RF implementa bootstraping para construir un Random Forest.Durante el entrenamiento (fit), para cada uno de los árboles del bosque, se genera un subconjunto del dataset original mediante muestreo con reemplazo ootstrap. Esto significa que, para cada árbol, se seleccionan aleatoriamente muestras del conjunto de entrenamiento, permitiendo que algunas se repitan y otras queden fuera. Cada árbol se entrena con su propio subconjunto, lo que introduce diversidad y reduce el sobreajuste.\n",
    "\n",
    "En la clase CART (definida en tree.py), utilizada por cada árbol del Random Forest, se incorpora la selección aleatoria de variables features en cada nodo. Esto se implementa en el método _grow_tree, donde para cada split se elige aleatoriamente un subconjunto de features para decidir la mejor división."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aa8ebf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9824561403508771\n"
     ]
    }
   ],
   "source": [
    "random_forest = RF(n_estimators=100, max_depth=5, min_samples_split=5, min_samples_leaf=2, max_features=5, bootstrap=True) \n",
    "random_forest.fit(X_train, y_train)\n",
    "y_pred = random_forest.predict(X_test)\n",
    "acc = np.mean(y_pred == y_test)\n",
    "print(f\"Test accuracy: {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fceebd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(X, y, k=5, max_depth=None, min_samples_split=2, min_samples_leaf=1, seed=40,n_estimators=100,max_features=30,bootstrap=False):\n",
    "    n_samples = len(y)\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(indices)  # Mezclar datos\n",
    "\n",
    "    fold_sizes = np.full(k, n_samples // k, dtype=int)\n",
    "    fold_sizes[: n_samples % k] += 1  # Repartir sobrantes\n",
    "    current = 0\n",
    "\n",
    "    scores_val = []\n",
    "    scores_train = []\n",
    "\n",
    "    for fold_size in fold_sizes:\n",
    "        start, stop = current, current + fold_size\n",
    "        val_idx = indices[start:stop]\n",
    "        train_idx = np.concatenate((indices[:start], indices[stop:]))\n",
    "\n",
    "        X_train_fold, y_train_fold = X[train_idx], y[train_idx]\n",
    "        X_val_fold, y_val_fold = X[val_idx], y[val_idx]\n",
    "\n",
    "\n",
    "        random_forest = RF(n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features, bootstrap)\n",
    "        random_forest.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "        y_pred_train= random_forest.predict(X_train_fold)\n",
    "        acc_train = np.mean(y_pred_train == y_train_fold)\n",
    "\n",
    "        y_pred_val = random_forest.predict(X_val_fold)\n",
    "        acc_val = np.mean(y_pred_val == y_val_fold)\n",
    "        # Predecir en entrenamiento y validación\n",
    "        scores_val.append(acc_val)\n",
    "        scores_train.append(acc_train)\n",
    "\n",
    "        current = stop\n",
    "\n",
    "    train = np.mean(scores_train)\n",
    "    val = np.mean(scores_val)\n",
    "    desvtrain = np.std(scores_train)\n",
    "    desvval = np.std(scores_val)\n",
    "\n",
    "    return train, val, desvtrain, desvval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d86e3e5",
   "metadata": {},
   "source": [
    "Para el Random Forest, se entrena el modelo con múltiples árboles y se evalúa la accuracy sobre el conjunto de test y mediante validación cruzada (k_fold_cross_validation). \n",
    "\n",
    "Para el random forest obtuvimos: \n",
    "Test accuracy: 0.9824561403508771\n",
    "Train accuracy: 0.9857142857142858 ± 0.004037620455137144\n",
    "Validation accuracy: 0.9824175824175825 ± 0.011206636293610524\n",
    "\n",
    "Mientras que para el decision Tree obtuvimos;\n",
    "Accuracy entrenamiento =  0.982 ± 0.007\n",
    "Accuracy validación =  0.938 ± 0.019\n",
    "Accuracy (en test): 0.9649122807017544\n",
    "\n",
    "Observamos entonces que en todos los casos el random Forest tiene un mejor desempeño que el decisons Tree. Esto se debe a que el Random Forest  muchos árboles, cada uno entrenado con diferentes subconjuntos de datos y variables. Esto reduce el sobreajuste y la variabilidad de las predicciones, y al promediar o votar entre varios árboles, el Random Forest generaliza mejor y es menos sensible al ruido o a particularidades del conjunto de entrenamiento.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2e05996b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9857142857142858 ± 0.004037620455137144\n",
      "Validation accuracy: 0.9824175824175825 ± 0.011206636293610524\n"
     ]
    }
   ],
   "source": [
    "score_train, score_val, desv_train, desv_val = k_fold_cross_validation(X_train, y_train, k=5, max_depth=5, min_samples_split=5, min_samples_leaf=2, seed=40,n_estimators=100,max_features=30,bootstrap=True)\n",
    "print(f\"Train accuracy: {score_train} ± {desv_train}\")\n",
    "print(f\"Validation accuracy: {score_val} ± {desv_val}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enttorno0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
